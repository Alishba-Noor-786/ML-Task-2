{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0fa1b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    BertTokenizer,\n",
    "    BertForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorWithPadding\n",
    ")\n",
    "import evaluate\n",
    "import gradio as gr\n",
    "\n",
    "# Load the AG News dataset\n",
    "dataset = load_dataset(\"ag_news\")\n",
    "\n",
    "# Load the BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Prepare datasets for PyTorch\n",
    "tokenized_datasets.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "train_dataset = tokenized_datasets[\"train\"]\n",
    "test_dataset = tokenized_datasets[\"test\"]\n",
    "\n",
    "# Load the BERT model for classification\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=4)\n",
    "\n",
    "# Load evaluation metrics\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "f1 = evaluate.load(\"f1\")\n",
    "\n",
    "# Define evaluation metrics function\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = logits.argmax(axis=1)\n",
    "    return {\n",
    "        \"accuracy\": accuracy.compute(predictions=predictions, references=labels),\n",
    "        \"f1\": f1.compute(predictions=predictions, references=labels, average=\"weighted\"),\n",
    "    }\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "# Create data collator\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# Create the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate the model\n",
    "trainer.evaluate()\n",
    "\n",
    "# Define inference function for Gradio\n",
    "def classify_news(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    predicted_class = torch.argmax(outputs.logits).item()\n",
    "    labels = [\"World\", \"Sports\", \"Business\", \"Sci/Tech\"]\n",
    "    return {labels[i]: float(outputs.logits[0][i]) for i in range(4)}\n",
    "\n",
    "# Create Gradio interface\n",
    "iface = gr.Interface(\n",
    "    fn=classify_news,\n",
    "    inputs=gr.Textbox(lines=2, placeholder=\"Enter a news headline...\"),\n",
    "    outputs=gr.Label(num_top_classes=4),\n",
    "    title=\"News Topic Classifier\",\n",
    "    description=\"Classify a news headline into World, Sports, Business, or Sci/Tech.\"\n",
    ")\n",
    "\n",
    "# Launch the Gradio app\n",
    "iface.launch()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a59a3c",
   "metadata": {},
   "source": [
    "\n",
    "# Task 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3625d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!pip install seaborn scikit-learn pandas matplotlib -q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e54239",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "import joblib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de5ac76",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "url = \"https://raw.githubusercontent.com/blastchar/telco-customer-churn/master/WA_Fn-UseC_-Telco-Customer-Churn.csv\"\n",
    "df = pd.read_csv(url)\n",
    "df = df[df[\"TotalCharges\"] != \" \"]\n",
    "df[\"TotalCharges\"] = df[\"TotalCharges\"].astype(float)\n",
    "df.drop([\"customerID\"], axis=1, inplace=True)\n",
    "df[\"Churn\"] = df[\"Churn\"].map({\"Yes\": 1, \"No\": 0})\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf182cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define categorical and numerical columns\n",
    "categorical_cols = df.select_dtypes(include=\"object\").columns.tolist()\n",
    "numerical_cols = df.select_dtypes(include=[\"int64\", \"float64\"]).columns.tolist()\n",
    "numerical_cols.remove(\"Churn\")\n",
    "\n",
    "# Preprocessing\n",
    "numeric_transformer = Pipeline(steps=[('scaler', StandardScaler())])\n",
    "categorical_transformer = Pipeline(steps=[('encoder', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numerical_cols),\n",
    "        ('cat', categorical_transformer, categorical_cols)\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85237017",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create full pipeline\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', LogisticRegression(max_iter=1000))\n",
    "])\n",
    "\n",
    "# Split dataset\n",
    "X = df.drop(\"Churn\", axis=1)\n",
    "y = df[\"Churn\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train\n",
    "pipeline.fit(X_train, y_train)\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11027d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# GridSearch for Random Forest\n",
    "pipeline_rf = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', RandomForestClassifier())\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    'classifier__n_estimators': [100, 200],\n",
    "    'classifier__max_depth': [5, 10]\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(pipeline_rf, param_grid, cv=3)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters:\", grid.best_params_)\n",
    "print(classification_report(y_test, grid.predict(X_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39996664",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save best model\n",
    "joblib.dump(grid.best_estimator_, \"telco_churn_pipeline.joblib\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e20f5c4",
   "metadata": {},
   "source": [
    "# Task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85740304",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install opendatasets scikit-learn tensorflow pandas matplotlib seaborn -q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5eca15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import opendatasets as od\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Concatenate, Input\n",
    "from tensorflow.keras.optimizers import Adam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbfa3a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download housing dataset from Kaggle\n",
    "od.download(\"https://www.kaggle.com/datasets/kumarkalyan/houses-dataset\")\n",
    "data_dir = \"houses-dataset\"\n",
    "df = pd.read_csv(os.path.join(data_dir, \"HousesInfo.txt\"))\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa3f0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter entries with existing images\n",
    "df = df[df[\"Image\"] != \" \"]\n",
    "df[\"Image\"] = df[\"Image\"].apply(lambda x: os.path.join(data_dir, \"Houses Dataset\", x))\n",
    "df = df[df[\"Image\"].apply(os.path.exists)].reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d042a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load VGG16 for image feature extraction\n",
    "IMG_SIZE = (224, 224)\n",
    "base_model = VGG16(weights=\"imagenet\", include_top=False, input_shape=(224, 224, 3))\n",
    "feature_extractor = Model(inputs=base_model.input, outputs=base_model.output)\n",
    "\n",
    "def extract_image_features(path):\n",
    "    img = load_img(path, target_size=IMG_SIZE)\n",
    "    img_array = img_to_array(img)\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    img_array = preprocess_input(img_array)\n",
    "    features = feature_extractor.predict(img_array, verbose=0)\n",
    "    return features.flatten()\n",
    "\n",
    "# Extract for first 100 samples for speed\n",
    "image_features = np.array([extract_image_features(p) for p in df[\"Image\"][:100]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1522207b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tabular data and price labels\n",
    "tabular_features = df[[\"Bedrooms\", \"Bathrooms\", \"Area\", \"Stories\"]].iloc[:100].values\n",
    "prices = df[\"Price\"].iloc[:100].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a1a7eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tab_train, X_tab_test, X_img_train, X_img_test, y_train, y_test = train_test_split(\n",
    "    tabular_features, image_features, prices, test_size=0.2, random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdcad905",
   "metadata": {},
   "outputs": [],
   "source": [
    "tab_input = Input(shape=(X_tab_train.shape[1],))\n",
    "img_input = Input(shape=(X_img_train.shape[1],))\n",
    "\n",
    "x1 = Dense(64, activation='relu')(tab_input)\n",
    "x2 = Dense(128, activation='relu')(img_input)\n",
    "\n",
    "combined = Concatenate()([x1, x2])\n",
    "z = Dense(64, activation='relu')(combined)\n",
    "z = Dense(1)(z)\n",
    "\n",
    "model = Model(inputs=[tab_input, img_input], outputs=z)\n",
    "model.compile(optimizer=Adam(1e-3), loss='mse')\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471787d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit([X_tab_train, X_img_train], y_train, epochs=10, batch_size=8, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a2cdc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict([X_tab_test, X_img_test])\n",
    "mae = mean_absolute_error(y_test, preds)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, preds))\n",
    "print(f\"MAE: {mae:.2f}, RMSE: {rmse:.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
